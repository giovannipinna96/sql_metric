# A Novel Metric for SQL Query Evaluation

## 1. Project Objective

This study aims to develop a new metric for evaluating SQL code, providing a granular assessment of query similarity.

## 2. Dataset

The [BIRD dev-dataset](https://bird-bench.github.io/) was selected for evaluation due to its unique characteristics as the first and only benchmark comprising real-world examples. It features multi-table structures with numerous rows and columns, covering diverse subjects.

## 3. Current Metrics and Proposed Enhancements

Existing metrics primarily assess three aspects:

- Query correctness: Comparison of gold and generated query strings.
- Result accuracy: Evaluation of exact equality between tables generated by gold and generated queries.
- Efficiency: Ratio of execution times between gold and generated queries.

Our proposed metric integrates these aspects into a comprehensive query correctness value.

## 4. Methodology

### 4.1 Semantic Similarity

We evaluate query semantic similarity through:

- String equality comparison
- Cosine similarity of query embeddings obtained from a linguistic model ([sergeyvi4ev/all-MiniLM-RAGSQL-code](https://huggingface.co/sergeyvi4ev/all-MiniLM-RAGSQL-code))

### 4.2 Result Correctness

Our approach extends beyond exact table matching:

- Considers row ordering when necessary
- Disregards column order and names
- For queries without ORDER BY, calculates minimum edit distance between tables (insertions, substitutions, deletions of columns and rows)
- For queries **with** ORDER BY, prioritizes row order while considering column order irrelevant.

$$
PartialScore = \begin{cases}

    1 - \frac{L_{gold} - L_{max-match}}{L_{gold}} &  \text{if} \  L_{gen} < L_{gold}\\ \\
    1 - \frac{\frac{L_{gold} - L_{max-match}}{L_{gold}}}{1.5} & \text{otherwise} 

\end{cases}
$$

Where $L$ can be the number of rows or the numbr of columns.



$$
FianlScore = \begin{cases}
     
    (PartialScore_{row} * 0.4 + PartialScore_{col} * 0.6) * 0.8 & \text{if} \ Tab_{gold} > Tab_{gen}
    \\
    PartialScore_{row} * 0.4 + PartialScore_{col} * 0.6 & \text{if} \ Tab_{gold} < Tab_{gen}
    \\
    0 & \text{if} \ Tab_{gold} * 1.25 > Tab_{gen}

\end{cases}
$$

where $Tab$ is the dimension of the table in term of number of row and columns. 
We give a 20% of penalty if the $TAB_{gen}$ (the table that come from the execution of the generate query) is smaller that the table generating form the gold query because this means that we miss some information.

Morover we give more importance to columns than rows. Infact, the columns have a weigth of $0.6$.

Finally, if the $Tab_{gen}$ is $25\%$ larger than the gold table we assign zero as the final score because there is too much noise to find the information we were really looking for.

### 4.3 Efficiency Evaluation

We maintain the existing method for efficiency assessment.

$$ VES = \sqrt{\frac{EX_{Q_{gold}}}{EX_{Q_{gen}}}} $$

where $EX$ is the execution time.

## 5. Model that we have

1. [C3_SQL](https://arxiv.org/pdf/2307.07306)
2. [DAILSQL](https://arxiv.org/pdf/2308.15363)
3. [DAILSQL_SC](https://arxiv.org/pdf/2406.01265v3)
4. [RESDSQL_3B_EK](https://arxiv.org/pdf/2302.05965)
5. [RESDSQL_Base_EK](https://arxiv.org/pdf/2302.05965)
6. [RESDSQL_Large_EK](https://arxiv.org/pdf/2302.05965)
7. [SFT_CodeS_1B_EK](https://arxiv.org/pdf/2402.16347)
8. [SFT_CodeS_3B_EK](https://arxiv.org/pdf/2402.16347)
9. [SFT_CodeS_7B_EK](https://arxiv.org/pdf/2402.16347)
10. [SFT_CodeS_15B_EK](https://arxiv.org/pdf/2402.16347)
11. [SuperSQL](https://dl.acm.org/doi/pdf/10.1145/276305.276389)

The SuperSQL deson't convince me.

## 6. Future Work

- [ ] **Multiple ORDER_BY** : big difference between 

        SELECT * FROM ( SELECT City, Street FROM schools ORDER BY City DESC LIMIT 211 ) subquery ORDER BY City ASC LIMIT 11 
        
    and
        
        SELECT * FROM ( SELECT City, Street FROM schools ORDER BY City DESC LIMIT 211 ) subquery City LIMIT 11
- [ ] **Weight Optimization**: Determine optimal weights for various operations in the scoring system.
- [ ] **Comparative Analysis**: Identify specific queries demonstrating the superiority of our metric over existing ones.
- [ ] **Threshold Evaluation**: Assess the validity of the current 25% difference threshold for default zero scores.
- [ ] **Penalty for not order by**: introduce a penalty also for not *ORDER_BY* queryes.
- [ ] **Write the paper**

## 7. Run the app.py

First run `pip install -r requirements.txt`

Second go in the `test_app` folder

Third run `python app.py`

Fourth go to the link with your browser.